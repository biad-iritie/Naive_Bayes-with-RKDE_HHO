{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing our novel approach with Naive Bayes\n",
    "\n",
    "> Preprocessing: Clean and preprocess your dataset. This may include handling missing values, encoding categorical variables, and scaling features.\n",
    "\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import data\n",
    "from libs import kde_lib\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from libs.exp_lib import Density_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris,load_breast_cancer\n",
    "from libs.RNB import RobustNaiveBayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outliers(X,y,outlier_proportion=.1):\n",
    "\n",
    "    # Calculate the number of outliers to add\n",
    "    num_outliers = int(outlier_proportion * len(X))\n",
    "    # Generate random outlier points within the range of the dataset\n",
    "    outliers_X = np.random.rand(num_outliers, 2) * (np.max(X, axis=0) - np.min(X, axis=0)) + np.min(X, axis=0)\n",
    "    outliers_y = np.array([1] * num_outliers)  # Assign a class label to outliers\n",
    "\n",
    "    # Concatenate outliers with the original dataset\n",
    "    X = np.vstack((X, outliers_X))\n",
    "    y = np.concatenate((y, outliers_y))\n",
    "    return X, y\n",
    "\n",
    "#X0, y0 = data.load_data_outlier(\"banana\") # OK\n",
    "\n",
    "# =======================================================\n",
    "#   Generate synthetic data with outliers\n",
    "# =======================================================\n",
    "#X0, y0 = make_circles(1500, noise=.1, random_state=42)\n",
    "num_dimensions = 2  # Number of dimensions\n",
    "outlier_proportion = .2\n",
    "#X0, y0 = make_classification(n_samples=2000, n_features=num_dimensions, n_informative=2, n_redundant=0,random_state=1, n_clusters_per_class=1) \n",
    "###### T distribution #########\n",
    "degrees_of_freedom = 1  # Degrees of freedom for the T-distribution\n",
    "sample_size = 3000  # Number of data points\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Generate data with a T-distribution\n",
    "X0 = np.random.standard_t(degrees_of_freedom, size=(sample_size, num_dimensions))\n",
    "\n",
    "# Define a threshold for binary classification\n",
    "threshold = 1.0  # You can adjust this threshold as needed\n",
    "\n",
    "# Assign labels based on the threshold\n",
    "y0 = (X0[:, 0] > threshold).astype(int)  # You can choose a different dimension for comparison\n",
    "\"\"\" # Print the first few data points and labels\n",
    "print(\"Data:\")\n",
    "print(X0[:5])\n",
    "print(\"\\nLabels:\")\n",
    "print(y0[:5]) \"\"\"\n",
    "###### End #########\n",
    "#X0, y0 = generate_outliers(X0, y0)\n",
    "\"\"\" dataset = load_iris()\n",
    "X0, y0 = dataset.data, dataset.target \"\"\"\n",
    "\"\"\" data = load_breast_cancer()\n",
    "X0, y0 = data.data, data.target\n",
    "selected_features = [0, 3] \n",
    "X0 = X0[:,selected_features] \"\"\"\n",
    "# Introduce outliers by modifying some data points\n",
    "num_outliers = int(outlier_proportion * len(X0))\n",
    "outliers_indices = np.random.choice(len(X0), num_outliers, replace=False)\n",
    "outliers = np.random.uniform(low=np.min(X0, axis=0)-10, high=np.max(X0, axis=0)+10, size=(num_outliers, num_dimensions))\n",
    "X0[outliers_indices] = outliers\n",
    "# Set labels for the outliers\n",
    "sep = int(len(outliers_indices)/2)\n",
    "y0[outliers_indices[:sep]] = 1 \n",
    "y0[outliers_indices[sep:]] = 0 \n",
    "\n",
    "#rng = np.random.RandomState(2)\n",
    "#X0 += 2 * rng.uniform(size=X0.shape)\n",
    "linearly_separable = (X0, y0)\n",
    "# =======================================================\n",
    "#   Done Generate  the synthetic data\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data with different distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Set the random seed for reproducibility\\nnp.random.seed(0)\\n\\n# Define the number of data points\\nnum_samples = 1000\\n\\n# Define the dimensions for data generation\\ndimensions = [2]\\n\\n# Parameters for the distributions\\ndistribution_params = {\\n    2 : {\\n    'Gaussian': {\\n        0: {'loc': [0, 0], 'scale': [3, 1.2]},\\n        1: { 'loc': [1, 1], 'scale': [3, 1.5]}\\n    }, \\n    'T': {\\n        0: {'df': 5, 'loc': [0, 0], 'scale': [2, .1]}},\\n        1: {'df': 5, 'loc': [2, 2], 'scale': [2, .1]}},\\n    \\n}\\n 'Cauchy': {\\n        1: {'loc': 0, 'scale': 1},\\n        2: {'loc': 0, 'scale': 1}},\\n    'Laplace': {'loc': 0, 'scale': 1}\\n# Distributions to generate data\\ndistributions = {\\n    'Gaussian': stats.norm,  \\n}\\n'T': stats.t,\\n    'Cauchy': stats.cauchy,\\n    'Laplace': stats.laplace  \""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the number of data points\n",
    "num_samples = 1000\n",
    "\n",
    "# Define the dimensions for data generation\n",
    "dimensions = [2]\n",
    "\n",
    "# Parameters for the distributions\n",
    "distribution_params = {\n",
    "    2 : {\n",
    "    'Gaussian': {\n",
    "        0: {'loc': [0, 0], 'scale': [3, 1.2]},\n",
    "        1: { 'loc': [1, 1], 'scale': [3, 1.5]}\n",
    "    }, \n",
    "    'T': {\n",
    "        0: {'df': 5, 'loc': [0, 0], 'scale': [2, .1]}},\n",
    "        1: {'df': 5, 'loc': [2, 2], 'scale': [2, .1]}},\n",
    "    \n",
    "}\n",
    " 'Cauchy': {\n",
    "        1: {'loc': 0, 'scale': 1},\n",
    "        2: {'loc': 0, 'scale': 1}},\n",
    "    'Laplace': {'loc': 0, 'scale': 1}\n",
    "# Distributions to generate data\n",
    "distributions = {\n",
    "    'Gaussian': stats.norm,  \n",
    "}\n",
    "'T': stats.t,\n",
    "    'Cauchy': stats.cauchy,\n",
    "    'Laplace': stats.laplace  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Generate data for each dimension and distribution\\nfor dim in dimensions:\\n    for name, distribution in distributions.items():\\n        # Get distribution parameters\\n        dict_params = distribution_params[dim][name]\\n        data = []\\n        # Generate data\\n        for index, params in dict_params.items():\\n            #data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\\n            data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\\n            print(data)\\n            #data = np.row_stack(data,distribution.rvs(size=(num_samples, dim), **params))\\n        # Create labels for binary classification\\n        labels = np.random.randint(2, size=num_samples)\\n        \\n        # Combine data and labels\\n        labeled_data = np.column_stack((data, labels))\\n        print(labeled_data) '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Generate data for each dimension and distribution\n",
    "for dim in dimensions:\n",
    "    for name, distribution in distributions.items():\n",
    "        # Get distribution parameters\n",
    "        dict_params = distribution_params[dim][name]\n",
    "        data = []\n",
    "        # Generate data\n",
    "        for index, params in dict_params.items():\n",
    "            #data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\n",
    "            data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\n",
    "            print(data)\n",
    "            #data = np.row_stack(data,distribution.rvs(size=(num_samples, dim), **params))\n",
    "        # Create labels for binary classification\n",
    "        labels = np.random.randint(2, size=num_samples)\n",
    "        \n",
    "        # Combine data and labels\n",
    "        labeled_data = np.column_stack((data, labels))\n",
    "        print(labeled_data) \"\"\"\n",
    "        \n",
    "        # Save or use the generated data for experiments\n",
    "        # For example, save it to a file or use it in your experiments\n",
    "        # np.savetxt(f'data_{name}_{dim}D_binary.csv', labeled_data, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X0, y0, test_size=.3, random_state=42)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "\n",
      "\n",
      "Accuracy 2: 0.6877777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' print(\"precision 2:\", precision)\\nprint(\"recall 2:\", recall)\\nprint(\"f1 2:\", f1) '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "\"\"\" print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Naive Bayes with HHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/03 06:11:21 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: Solving 2-objective optimization problem with weights: [1 1].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create and fit the RobustNaiveBayes classifier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m RobustNaiveBayes()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(predictions))\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/RNB.py:59\u001b[0m, in \u001b[0;36mRobustNaiveBayes.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifiers[class_label]\u001b[39m.\u001b[39mfit(class_data, y[class_indices])\n\u001b[1;32m     56\u001b[0m \u001b[39m\"\"\" if self.h_selection == \"hoo\":\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    bandwidth = kde_lib.hho_bandwith_selection(\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m        class_data, class_data) \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m bandwidth \u001b[39m=\u001b[39m kde_lib\u001b[39m.\u001b[39;49mselection_bandwidth(\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh_selection, class_data)\n\u001b[1;32m     61\u001b[0m model \u001b[39m=\u001b[39m Density_model(\u001b[39m\"\u001b[39m\u001b[39mrkde\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, bandwidth)\n\u001b[1;32m     62\u001b[0m model\u001b[39m.\u001b[39mfit(class_data, class_data, grid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/kde_lib.py:363\u001b[0m, in \u001b[0;36mselection_bandwidth\u001b[0;34m(sel, data)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselection_bandwidth\u001b[39m(sel: \u001b[39mstr\u001b[39m, data):\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m sel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhho\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 363\u001b[0m         \u001b[39mreturn\u001b[39;00m hho_bandwith_selection(data)\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m sel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpso\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    365\u001b[0m         \u001b[39mreturn\u001b[39;00m pso_bandwidth_selection(data)\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/kde_lib.py:349\u001b[0m, in \u001b[0;36mhho_bandwith_selection\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    347\u001b[0m pop_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m  \u001b[39m# 10 number of population size\u001b[39;00m\n\u001b[1;32m    348\u001b[0m model \u001b[39m=\u001b[39m OriginalHHO(epoch, pop_size)\n\u001b[0;32m--> 349\u001b[0m best_position, best_fitness \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msolve(problem_multi)\n\u001b[1;32m    350\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResult hho_bandwith_selection: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(best_position))\n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\" print(\"\\n\")\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mprint(len(model.history.list_population)) \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:275\u001b[0m, in \u001b[0;36mOptimizer.solve\u001b[0;34m(self, problem, mode, starting_positions, n_workers, termination)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize_variables()\n\u001b[1;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_initialization(starting_positions)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitialization()\n\u001b[1;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_initialization()\n\u001b[1;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_main_loop()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:136\u001b[0m, in \u001b[0;36mOptimizer.initialization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialization\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_population(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpop_size)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:358\u001b[0m, in \u001b[0;36mOptimizer.create_population\u001b[0;34m(self, pop_size)\u001b[0m\n\u001b[1;32m    356\u001b[0m             pop\u001b[39m.\u001b[39mappend(f\u001b[39m.\u001b[39mresult())\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     pop \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_solution(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mlb, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mub) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, pop_size)]\n\u001b[1;32m    359\u001b[0m \u001b[39mreturn\u001b[39;00m pop\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:358\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    356\u001b[0m             pop\u001b[39m.\u001b[39mappend(f\u001b[39m.\u001b[39mresult())\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     pop \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_solution(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mlb, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mub) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, pop_size)]\n\u001b[1;32m    359\u001b[0m \u001b[39mreturn\u001b[39;00m pop\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:189\u001b[0m, in \u001b[0;36mOptimizer.create_solution\u001b[0;34m(self, lb, ub, pos)\u001b[0m\n\u001b[1;32m    187\u001b[0m     pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_position(lb, ub)\n\u001b[1;32m    188\u001b[0m position \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamend_position(pos, lb, ub)\n\u001b[0;32m--> 189\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_target_wrapper(position)\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m [position, target]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/mealpy/optimizer.py:164\u001b[0m, in \u001b[0;36mOptimizer.get_target_wrapper\u001b[0;34m(self, position, counted)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m counted:\n\u001b[1;32m    163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfe_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 164\u001b[0m objs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mfit_func(position)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mobj_is_list:\n\u001b[1;32m    166\u001b[0m     objs \u001b[39m=\u001b[39m [objs]\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/kde_lib.py:330\u001b[0m, in \u001b[0;36mHHO_BandwidthSelection.fit_func\u001b[0;34m(self, solution)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_func\u001b[39m(\u001b[39mself\u001b[39m, solution):\n\u001b[0;32m--> 330\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj_func1(solution), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj_func2(solution)]\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/kde_lib.py:324\u001b[0m, in \u001b[0;36mHHO_BandwidthSelection.obj_func1\u001b[0;34m(self, bandwidth)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func1\u001b[39m(\u001b[39mself\u001b[39m, bandwidth):\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mreturn\u001b[39;00m bcv_objective(bandwidth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/Desktop/THESIS/Tests/RKDE_HHO/libs/kde_lib.py:269\u001b[0m, in \u001b[0;36mbcv_objective\u001b[0;34m(bandwidth, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m# Calculate the density estimate without x_i\u001b[39;00m\n\u001b[1;32m    268\u001b[0m kde\u001b[39m.\u001b[39mfit(x_rest)\n\u001b[0;32m--> 269\u001b[0m density_rest \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(kde\u001b[39m.\u001b[39;49mscore_samples(x_i\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[1;32m    270\u001b[0m \u001b[39m# Calculate BCV for x_i\u001b[39;00m\n\u001b[1;32m    271\u001b[0m bcv_values\u001b[39m.\u001b[39mappend(density_rest)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_science/lib/python3.9/site-packages/sklearn/neighbors/_kde.py:270\u001b[0m, in \u001b[0;36mKernelDensity.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m     N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39msum_weight\n\u001b[1;32m    269\u001b[0m atol_N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matol \u001b[39m*\u001b[39m N\n\u001b[0;32m--> 270\u001b[0m log_density \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_\u001b[39m.\u001b[39;49mkernel_density(\n\u001b[1;32m    271\u001b[0m     X,\n\u001b[1;32m    272\u001b[0m     h\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbandwidth_,\n\u001b[1;32m    273\u001b[0m     kernel\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel,\n\u001b[1;32m    274\u001b[0m     atol\u001b[39m=\u001b[39;49matol_N,\n\u001b[1;32m    275\u001b[0m     rtol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrtol,\n\u001b[1;32m    276\u001b[0m     breadth_first\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbreadth_first,\n\u001b[1;32m    277\u001b[0m     return_log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    278\u001b[0m )\n\u001b[1;32m    279\u001b[0m log_density \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(N)\n\u001b[1;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m log_density\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create and fit the RobustNaiveBayes classifier\n",
    "model = RobustNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "\"\"\" print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model VS other classification models\n",
    "Compare the Naive Bayes classifier with Optimized Robust Kernel Density Estimation to other classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = [\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Naive Bayes with RKDE',RobustNaiveBayes()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear')),\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=3)),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('K-Means Clustering', KMeans(n_clusters=3))\n",
    "]\n",
    "\n",
    "#Compare classifier performances\n",
    "results = []\n",
    "for name,classifer in classifiers:\n",
    "    clf = Pipeline([('classifier', classifer)])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append((name, accuracy))\n",
    "\n",
    "for name, accuracy in results:\n",
    "    print(f'{name}: Accuracy = {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the RobustNaiveBayes classifier\n",
    "\"\"\" model = RobustNaiveBayes(\"pso\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers Comparaison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "#   Generate synthetic data with outliers\n",
    "# =======================================================\n",
    "#make_moons = make_moons(500, noise=.2, random_state=42)\n",
    "\n",
    "#X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,random_state=1, n_clusters_per_class=1) \n",
    "\n",
    " # Introduce outliers by modifying some data points\n",
    "\"\"\" outlier_proportion = .1\n",
    "num_outliers = int(outlier_proportion * len(X0))\n",
    "outliers_indices = np.random.choice(len(X0), num_outliers, replace=False)\n",
    "outliers = np.random.uniform(low=np.min(X0, axis=0)-1, high=np.max(X0, axis=0)+1, size=(num_outliers, 2))\n",
    "X0[outliers_indices] = outliers\n",
    "# Set labels for the outliers\n",
    "y0[outliers_indices] = 1  \"\"\"\n",
    "#X0, y0 = generate_outliers(X0, y0)\n",
    "\"\"\" data = load_breast_cancer()\n",
    "X0, y0 = data.data, data.target\n",
    "selected_features = [0, 3] \n",
    "X0 = X0[:,selected_features] \"\"\"\n",
    "\n",
    "\"\"\" rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)  \"\"\"\n",
    "#X, y = data.load_data(\"banana\")\n",
    "# =======================================================\n",
    "#   Done Generate  the synthetic data\n",
    "# =======================================================\n",
    "#X0, y0 = data.load_data_outlier(\"banana\")\n",
    "linearly_separable = (X0, y0)\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"RNB with HHO\": RobustNaiveBayes(),\n",
    "    #\"RNB with PSO\": RobustNaiveBayes(\"pso\"),\n",
    "}\n",
    "\n",
    "\n",
    "#iris = load_iris()\n",
    "\n",
    "datasets = [\n",
    "    linearly_separable\n",
    "    \n",
    "]\n",
    "datasets_name = [\n",
    "    #\"make_moons\",\n",
    "   # \"make_circles \",\n",
    "    \"Synthetic\",\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#2ca02c\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) +1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
    "    # Plot the Outliers points\n",
    "    ax.scatter(X[outliers_indices, 0], X[outliers_indices, 1],marker=\"X\", c=y[outliers_indices], edgecolors=\"r\")\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in classifiers.items():\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        # Create a meshgrid for plotting\n",
    "        h = .02  # Step size in the mesh\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        #clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions using the classifier\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=.5)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \"\"\" DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=.8, ax=ax, eps=.5\n",
    "        ) \"\"\"\n",
    "        \n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        # Plot the Outliers points\n",
    "        ax.scatter(X[outliers_indices, 0], X[outliers_indices, 1],marker=\"X\", c=y[outliers_indices], edgecolors=\"r\")\n",
    "        \n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"Ac: {}\".format((\"%.2f\" % accuracy).lstrip(\"0\"))),\n",
    "            size=10,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "643985f69f89cfee59ccb9ceb257d9cb58c13bdd97c74d508c0964730dccc282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
