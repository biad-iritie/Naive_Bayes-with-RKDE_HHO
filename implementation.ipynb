{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing our novel approach with Naive Bayes\n",
    "\n",
    "> Preprocessing: Clean and preprocess your dataset. This may include handling missing values, encoding categorical variables, and scaling features.\n",
    "\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import data\n",
    "from libs import kde_lib\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from libs.exp_lib import Density_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris,load_breast_cancer\n",
    "from libs.RNB import RobustNaiveBayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_outliers(X,y,outlier_proportion=.1):\n",
    "\n",
    "    # Calculate the number of outliers to add\n",
    "    num_outliers = int(outlier_proportion * len(X))\n",
    "    # Generate random outlier points within the range of the dataset\n",
    "    outliers_X = np.random.rand(num_outliers, 2) * (np.max(X, axis=0) - np.min(X, axis=0)) + np.min(X, axis=0)\n",
    "    outliers_y = np.array([1] * num_outliers)  # Assign a class label to outliers\n",
    "\n",
    "    # Concatenate outliers with the original dataset\n",
    "    X = np.vstack((X, outliers_X))\n",
    "    y = np.concatenate((y, outliers_y))\n",
    "    return X, y\n",
    "\n",
    "#X0, y0 = data.load_data_outlier(\"banana\") # OK\n",
    "\n",
    "# =======================================================\n",
    "#   Generate synthetic data with outliers\n",
    "# =======================================================\n",
    "#X0, y0 = make_circles(1000, noise=.1, random_state=42)\n",
    "X0, y0 = make_classification(n_samples=2000, n_features=3, n_informative=2, n_redundant=0,random_state=1, n_clusters_per_class=1) \n",
    "\n",
    "#X0, y0 = generate_outliers(X0, y0)\n",
    "\"\"\" dataset = load_iris()\n",
    "X0, y0 = dataset.data, dataset.target \"\"\"\n",
    "\"\"\" data = load_breast_cancer()\n",
    "X0, y0 = data.data, data.target\n",
    "selected_features = [0, 3] \n",
    "X0 = X0[:,selected_features] \"\"\"\n",
    "# Introduce outliers by modifying some data points\n",
    "outlier_proportion = .25\n",
    "num_outliers = int(outlier_proportion * len(X0))\n",
    "outliers_indices = np.random.choice(len(X0), num_outliers, replace=False)\n",
    "outliers = np.random.uniform(low=np.min(X0, axis=0)-1, high=np.max(X0, axis=0)+1, size=(num_outliers, 3))\n",
    "X0[outliers_indices] = outliers\n",
    "# Set labels for the outliers\n",
    "sep = int(len(outliers_indices)/2)\n",
    "y0[outliers_indices[:sep]] = 1 \n",
    "y0[outliers_indices[sep:]] = 0 \n",
    "\n",
    "\"\"\" rng = np.random.RandomState(2)\n",
    "X0 += 2 * rng.uniform(size=X0.shape) \"\"\"\n",
    "linearly_separable = (X0, y0)\n",
    "# =======================================================\n",
    "#   Done Generate  the synthetic data\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data with different distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Set the random seed for reproducibility\\nnp.random.seed(0)\\n\\n# Define the number of data points\\nnum_samples = 1000\\n\\n# Define the dimensions for data generation\\ndimensions = [2]\\n\\n# Parameters for the distributions\\ndistribution_params = {\\n    2 : {\\n    'Gaussian': {\\n        0: {'loc': [0, 0], 'scale': [3, 1.2]},\\n        1: { 'loc': [1, 1], 'scale': [3, 1.5]}\\n    }, \\n    'T': {\\n        0: {'df': 5, 'loc': [0, 0], 'scale': [2, .1]}},\\n        1: {'df': 5, 'loc': [2, 2], 'scale': [2, .1]}},\\n    \\n}\\n 'Cauchy': {\\n        1: {'loc': 0, 'scale': 1},\\n        2: {'loc': 0, 'scale': 1}},\\n    'Laplace': {'loc': 0, 'scale': 1}\\n# Distributions to generate data\\ndistributions = {\\n    'Gaussian': stats.norm,  \\n}\\n'T': stats.t,\\n    'Cauchy': stats.cauchy,\\n    'Laplace': stats.laplace  \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the number of data points\n",
    "num_samples = 1000\n",
    "\n",
    "# Define the dimensions for data generation\n",
    "dimensions = [2]\n",
    "\n",
    "# Parameters for the distributions\n",
    "distribution_params = {\n",
    "    2 : {\n",
    "    'Gaussian': {\n",
    "        0: {'loc': [0, 0], 'scale': [3, 1.2]},\n",
    "        1: { 'loc': [1, 1], 'scale': [3, 1.5]}\n",
    "    }, \n",
    "    'T': {\n",
    "        0: {'df': 5, 'loc': [0, 0], 'scale': [2, .1]}},\n",
    "        1: {'df': 5, 'loc': [2, 2], 'scale': [2, .1]}},\n",
    "    \n",
    "}\n",
    " 'Cauchy': {\n",
    "        1: {'loc': 0, 'scale': 1},\n",
    "        2: {'loc': 0, 'scale': 1}},\n",
    "    'Laplace': {'loc': 0, 'scale': 1}\n",
    "# Distributions to generate data\n",
    "distributions = {\n",
    "    'Gaussian': stats.norm,  \n",
    "}\n",
    "'T': stats.t,\n",
    "    'Cauchy': stats.cauchy,\n",
    "    'Laplace': stats.laplace  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Generate data for each dimension and distribution\\nfor dim in dimensions:\\n    for name, distribution in distributions.items():\\n        # Get distribution parameters\\n        dict_params = distribution_params[dim][name]\\n        data = []\\n        # Generate data\\n        for index, params in dict_params.items():\\n            #data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\\n            data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\\n            print(data)\\n            #data = np.row_stack(data,distribution.rvs(size=(num_samples, dim), **params))\\n        # Create labels for binary classification\\n        labels = np.random.randint(2, size=num_samples)\\n        \\n        # Combine data and labels\\n        labeled_data = np.column_stack((data, labels))\\n        print(labeled_data) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Generate data for each dimension and distribution\n",
    "for dim in dimensions:\n",
    "    for name, distribution in distributions.items():\n",
    "        # Get distribution parameters\n",
    "        dict_params = distribution_params[dim][name]\n",
    "        data = []\n",
    "        # Generate data\n",
    "        for index, params in dict_params.items():\n",
    "            #data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\n",
    "            data = (np.vstack((data,distribution.rvs(size=(num_samples, dim)))), distribution.rvs(size=(num_samples, dim), **params))[len(data) == 0]\n",
    "            print(data)\n",
    "            #data = np.row_stack(data,distribution.rvs(size=(num_samples, dim), **params))\n",
    "        # Create labels for binary classification\n",
    "        labels = np.random.randint(2, size=num_samples)\n",
    "        \n",
    "        # Combine data and labels\n",
    "        labeled_data = np.column_stack((data, labels))\n",
    "        print(labeled_data) \"\"\"\n",
    "        \n",
    "        # Save or use the generated data for experiments\n",
    "        # For example, save it to a file or use it in your experiments\n",
    "        # np.savetxt(f'data_{name}_{dim}D_binary.csv', labeled_data, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X0, y0, test_size=0.3, random_state=42)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "\n",
      "\n",
      "Accuracy 2: 0.8933333333333333\n",
      "precision 2: 0.8908450704225352\n",
      "recall 2: 0.8846153846153846\n",
      "f1 2: 0.887719298245614\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Naive Bayes with HHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/18 03:20:45 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: Solving 2-objective optimization problem with weights: [0.5 0.5].\n",
      "2023/10/18 03:20:53 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 1, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 4.95283 seconds\n",
      "2023/10/18 03:20:59 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 2, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 5.36783 seconds\n",
      "2023/10/18 03:21:04 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 3, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 5.66406 seconds\n",
      "2023/10/18 03:21:09 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 4, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 5.07828 seconds\n",
      "2023/10/18 03:21:15 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 5, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 5.32737 seconds\n",
      "2023/10/18 03:21:21 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 6, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.61518 seconds\n",
      "2023/10/18 03:21:28 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 7, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.48000 seconds\n",
      "2023/10/18 03:21:34 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 8, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.44887 seconds\n",
      "2023/10/18 03:21:41 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 9, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 7.08402 seconds\n",
      "2023/10/18 03:21:46 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 10, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 4.83590 seconds\n",
      "2023/10/18 03:21:53 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 11, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.48140 seconds\n",
      "2023/10/18 03:21:59 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 12, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.80203 seconds\n",
      "2023/10/18 03:22:05 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 13, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 5.17482 seconds\n",
      "2023/10/18 03:22:11 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 14, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.83470 seconds\n",
      "2023/10/18 03:22:18 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 15, Current best: 0.0003192021793309303, Global best: 0.0003192021793309303, Runtime: 6.46394 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result hho_bandwith_selection: [0.99]\n",
      "Stop at 5 iterations\n",
      "Stop at 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/18 03:22:18 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: Solving 2-objective optimization problem with weights: [0.5 0.5].\n",
      "2023/10/18 03:22:26 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 1, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 4.48576 seconds\n",
      "2023/10/18 03:22:31 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 2, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 4.52959 seconds\n",
      "2023/10/18 03:22:37 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 3, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.11537 seconds\n",
      "2023/10/18 03:22:42 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 4, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 4.84412 seconds\n",
      "2023/10/18 03:22:47 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 5, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 5.84954 seconds\n",
      "2023/10/18 03:22:53 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 6, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.13663 seconds\n",
      "2023/10/18 03:23:00 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 7, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.01165 seconds\n",
      "2023/10/18 03:23:06 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 8, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.61935 seconds\n",
      "2023/10/18 03:23:13 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 9, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.51384 seconds\n",
      "2023/10/18 03:23:20 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 10, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 7.50142 seconds\n",
      "2023/10/18 03:23:27 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 11, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 7.17632 seconds\n",
      "2023/10/18 03:23:35 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 12, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 7.32805 seconds\n",
      "2023/10/18 03:23:43 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 13, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 8.13485 seconds\n",
      "2023/10/18 03:23:49 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 14, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 6.39862 seconds\n",
      "2023/10/18 03:23:56 PM, INFO, mealpy.swarm_based.HHO.OriginalHHO: >Problem: P, Epoch: 15, Current best: 0.0002641291721687006, Global best: 0.0002641291721687006, Runtime: 7.01291 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result hho_bandwith_selection: [0.99]\n",
      "Stop at 5 iterations\n",
      "Stop at 100 iterations\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Accuracy 2: 0.9\n",
      "precision 2: 0.8741721854304636\n",
      "recall 2: 0.9230769230769231\n",
      "f1 2: 0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the RobustNaiveBayes classifier\n",
    "model = RobustNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model VS other classification models\n",
    "Compare the Naive Bayes classifier with Optimized Robust Kernel Density Estimation to other classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = [\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Naive Bayes with RKDE',RobustNaiveBayes()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear')),\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=3)),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('K-Means Clustering', KMeans(n_clusters=3))\n",
    "]\n",
    "\n",
    "#Compare classifier performances\n",
    "results = []\n",
    "for name,classifer in classifiers:\n",
    "    clf = Pipeline([('classifier', classifer)])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results.append((name, accuracy))\n",
    "\n",
    "for name, accuracy in results:\n",
    "    print(f'{name}: Accuracy = {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the RobustNaiveBayes classifier\n",
    "\"\"\" model = RobustNaiveBayes(\"pso\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(np.unique(predictions))\n",
    "\n",
    "print(\"\\n\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions )\n",
    "print(\"Accuracy 2:\", accuracy)\n",
    "print(\"precision 2:\", precision)\n",
    "print(\"recall 2:\", recall)\n",
    "print(\"f1 2:\", f1) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers Comparaison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "#   Generate synthetic data with outliers\n",
    "# =======================================================\n",
    "#make_moons = make_moons(500, noise=.2, random_state=42)\n",
    "\n",
    "#X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,random_state=1, n_clusters_per_class=1) \n",
    "\n",
    " # Introduce outliers by modifying some data points\n",
    "\"\"\" outlier_proportion = .1\n",
    "num_outliers = int(outlier_proportion * len(X0))\n",
    "outliers_indices = np.random.choice(len(X0), num_outliers, replace=False)\n",
    "outliers = np.random.uniform(low=np.min(X0, axis=0)-1, high=np.max(X0, axis=0)+1, size=(num_outliers, 2))\n",
    "X0[outliers_indices] = outliers\n",
    "# Set labels for the outliers\n",
    "y0[outliers_indices] = 1  \"\"\"\n",
    "#X0, y0 = generate_outliers(X0, y0)\n",
    "\"\"\" data = load_breast_cancer()\n",
    "X0, y0 = data.data, data.target\n",
    "selected_features = [0, 3] \n",
    "X0 = X0[:,selected_features] \"\"\"\n",
    "\n",
    "\"\"\" rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)  \"\"\"\n",
    "#X, y = data.load_data(\"banana\")\n",
    "# =======================================================\n",
    "#   Done Generate  the synthetic data\n",
    "# =======================================================\n",
    "#X0, y0 = data.load_data_outlier(\"banana\")\n",
    "linearly_separable = (X0, y0)\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"RNB with HHO\": RobustNaiveBayes(),\n",
    "    #\"RNB with PSO\": RobustNaiveBayes(\"pso\"),\n",
    "}\n",
    "\n",
    "\n",
    "#iris = load_iris()\n",
    "\n",
    "datasets = [\n",
    "    linearly_separable\n",
    "    \n",
    "]\n",
    "datasets_name = [\n",
    "    #\"make_moons\",\n",
    "   # \"make_circles \",\n",
    "    \"Synthetic\",\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#2ca02c\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) +1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
    "    # Plot the Outliers points\n",
    "    ax.scatter(X[outliers_indices, 0], X[outliers_indices, 1],marker=\"X\", c=y[outliers_indices], edgecolors=\"r\")\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in classifiers.items():\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        # Create a meshgrid for plotting\n",
    "        h = .02  # Step size in the mesh\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        #clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions using the classifier\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=.5)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \"\"\" DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=.8, ax=ax, eps=.5\n",
    "        ) \"\"\"\n",
    "        \n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        # Plot the Outliers points\n",
    "        ax.scatter(X[outliers_indices, 0], X[outliers_indices, 1],marker=\"X\", c=y[outliers_indices], edgecolors=\"r\")\n",
    "        \n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"Ac: {}\".format((\"%.2f\" % accuracy).lstrip(\"0\"))),\n",
    "            size=10,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "643985f69f89cfee59ccb9ceb257d9cb58c13bdd97c74d508c0964730dccc282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
